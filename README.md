<!DOCTYPE html> <html lang="en"> <head>     <meta charset="UTF-8">     <meta name="viewport" content="width=device-width, initial-scale=1.0">     <title>OmniNA</title> </head> <body>     <div align="center">         <h1> üåê OmniNA</h1>         <p>A Foundation Model for Nucleotide Sequences</p>     </div>      <h2>üìã Overview</h2>     <p>         OmniNA is a foundation generative model designed for comprehensive nucleotide sequence learning.          OmniNA is pre-trained on all nucleotide sequences and the corresponding annotations obtained from NCBI nucleotide database.          The model can be fine-tuned for various genomic tasks.     </p>      <h2>‚ú® Features</h2>     <ul>         <li>Pre-trained on 91.7 million nucleotide sequences from various species.</li>         <li>Fine-tuned across 17 tasks, achieving state-of-the-art performance.</li>         <li>Designed to be scalable and accessible to users of all expertise levels.</li>     </ul>      <h2>üîó Model Variants</h2>     <p>OmniNA comes in three variants with different parameter scales to meet various needs:</p>     <table>         <thead>             <tr>                 <th>Model Variant</th>                 <th>Number of Layers</th>                 <th>Number of Heads</th>                 <th>Number of Parameters</th>             </tr>         </thead>         <tbody>             <tr>                 <td>OmniNA-66M</td>                 <td>8</td>                 <td>8</td>                 <td>66 Million</td>             </tr>             <tr>                 <td>OmniNA-330M</td>                 <td>16</td>                 <td>16</td>                 <td>330 Million</td>             </tr>             <tr>                 <td>OmniNA-1.7B</td>                 <td>24</td>                 <td>32</td>                 <td>1.7 Billion</td>             </tr>         </tbody>     </table>      <h2>üî® Installation</h2>     <p>Required packages can be installed using:</p>     <pre><code>pip install datasets
pip install torch
pip install transformers
pip install accelerate
pip install tensorboardX</code></pre>     <p>To utilize the pre-trained OmniNA model, begin by cloning the Hugging Face repository:</p>     <pre><code>XLS/OmniNA-220m
XLS/OmniNA-66m 
## OmniNA-1.3b will be available soon.</code></pre>      <h2>üå∏ Training</h2>     <h3>Prepare Data</h3>
<p>Following these steps to prepare your data:</p>
<ol>
    <li><strong>Collecting Data:</strong> Map nucleotide sequences to their corresponding annotations.</li>
    <li><strong>Chunk Sequences:</strong> Ensure the sequences are between 200 bp and 3000 bp in length.</li>
    <li><strong>Formatting Data:</strong> You should prepare the data as a .csv file for both pretraining and fine-tuning tasks. Here's how you can organize the data:</li>
    <ul>
        <li><strong>For pretraining (two columns):</strong></li>
        <ul>
            <li><strong>sequence:</strong> The nucleotide sequence.</li>
            <li><strong>response:</strong> The corresponding sequence annotation.</li>
        </ul>
        <p>For example:</p>
        <table>
            <thead>
                <tr>
                    <th>sequence</th>
                    <th>response</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>ATGCATGCATGC</td>
                    <td>Annotation A</td>
                </tr>
                <tr>
                    <td>GCTAGCTAGCTA</td>
                    <td>Annotation B</td>
                </tr>
                <tr>
                    <td>TAGCTAGCTAGC</td>
                    <td>Annotation C</td>
                </tr>
            </tbody>
        </table>
        <li><strong>For pretraining (three columns):</strong></li>
        <ul>
            <li><strong>sequence:</strong> The nucleotide sequence.</li>
            <li><strong>question:</strong> The question related to the sequence.</li>
            <li><strong>response:</strong> The corresponding answer.</li>
        </ul>
        <p>For example:</p>
        <table>
            <thead>
                <tr>
                    <th>sequence</th>
                    <th>question</th>
                    <th>response</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>ATGCATGCATGC</td>
                    <td>Is the sequence identified as a promoter?</td>
                    <td>Yes</td>
                </tr>
                <tr>
                    <td>GCTAGCTAGCTA</td>
                    <td>Is the sequence identified as an enhancer?</td>
                    <td>No</td>
                </tr>
            </tbody>
        </table>
    </ul>
</ol>

<h3>Pretrain</h3>
<p>
    <code>bash pretrain.sh</code>
</p>
<p>Output of first stage of training will be saved in `output/pretrain/`.</p>

<h3>Finetune</h3>
<p>
    <code>bash finetune.sh</code>
</p>
<p>The final trained model checkpoint will be saved in `output/finetune`.</p>

<h2>üìö Citation</h2>     
<p>If you use OmniNA in your research, please cite our work:</p>     
<pre><code>@article{omniNA2023,
title={OmniNA: A Foundation Model for Nucleotide Sequences},
author={John Doe and Jane Smith},
journal={Bioinformatics Journal},
year={2023},
volume={10},
pages={123-134}}</code></pre>      

<h2>üåü Acknowledgement</h2>     
<p>The codebase we built upon is adapted from <a href="https://github.com/meta-llama/llama">Llama</a>. We thank the original authors for their valuable contributions!</p>     
<h2>üìÑ License</h2>     
<p>This project is licensed under the MIT License - see the <a href="LICENSE">LICENSE</a> file for details.</p> 
</body> 
</html>


